---
title: "Untitled"
output: html_document
---

```{r setup, include=FALSE}
# inspiracao https://github.com/prodipta/R-examples/blob/master/wide_and_deep.R
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(keras)
tf <- reticulate::import("tensorflow")

train <- read.csv("train.csv")
test <- read.csv("test.csv")

```

```{r}
# preprocess_data <- function(data, wide_cols, deep_cols, y){
#   
#   return(
#     list(
#       train=list(wide=data$train[,wide_cols],
#                 deep=data$train[,deep_cols],
#                 y=y),
#       test=list(wide=data$test[,wide_cols],
#                deep=data$test[,deep_cols],
#                y=NA)))
# }
```

```{r}
x_train <- train %>% select(-id, -target)
x_test <- test %>% select(-id)

y <- str_extract(train$target, "\\d") %>% as.numeric()
y <- to_categorical(y)[, 2:10]

# data <- list(train = x_train, test = x_test)
```

```{r}
# wide_cols <- colnames(x_train)[which(map_lgl(x_train, ~length(unique(.x))>=100))]
# deep_cols <- colnames(x_train)[which(map_lgl(x_train, ~length(unique(.x))<100))]

# data <- preprocess_data(data, wide_cols, deep_cols, y)
```

```{r}
# size_deep <- NCOL(data$train$deep)
# size_wide <- NCOL(data$train$wide)
input_size <- ncol(x_train)
epoch <- 100
batchsize <- 128
folds <- 8
```

```{r}
early_stopping <- callback_early_stopping(
  monitor = "val_loss",
  min_delta = 0.0000001,
  patience = 15,
  restore_best_weights = TRUE
)

plateau <- callback_reduce_lr_on_plateau(
  monitor = "val_loss",
  factor = 0.05,
  patience = 2,
  verbose = 1,
  min_delta = 0.0000001,
  cooldown = 0,
  min_lr = 0
)
```

```{r}
instance_model <- function(){

  # wide <- layer_input(size_wide)
  
  wide <- tf$keras$experimental$LinearModel()
  wide$build(input_size)
  
  inputs <- layer_input(input_size) 
  
  x <- inputs %>% 
    layer_embedding(input_dim = 360, output_dim =  8, input_length = input_size) %>%
    
    layer_conv_1d(filters = 16, kernel_size = 1, activation = 'relu') %>%
    layer_flatten() %>%
    layer_dropout(0.3) %>%
    
    layer_dense(units = 128, activation = 'relu') %>% 
    layer_batch_normalization() %>% 
    layer_dropout(0.3) %>%
    
    layer_dense(units = 64, activation = 'relu') %>% 
    layer_batch_normalization() %>% 
    layer_dropout(0.3) %>%
    
    layer_dense(units = 32, activation = 'relu') %>% 
    layer_batch_normalization() %>% 
    layer_dropout(0.2)
  
  outputs <-  x %>% layer_dense(units = 9)
  
  deep <- keras_model(inputs = inputs, outputs = outputs)
  
  wide_n_deep <- tf$keras$experimental$WideDeepModel(wide, deep, activation='softmax')
  
  wide_n_deep$build(input_shape = list(input_size, input_size))
  
  wide_n_deep
  
  return(wide_n_deep)
}
```


```{r}
wide_n_deep <- instance_model()

wide_n_deep %>%
  compile(
    loss = 'categorical_crossentropy',
    optimizer = optimizer_adam(
      lr = 0.0002) )
```

```{r}
history <- wide_n_deep %>% fit(
  x = as.matrix(x_train),
  y = y,
  batch_size = batchsize,
  epochs = epoch,
  validation_split=0.2,
  callbacks = c(early_stopping, plateau)
)
```

```{r}
history
```

```{r}
plot(history, na.rm = TRUE)
```



```{r}
set.seed(314)
kf <- caret::createFolds(train$target, folds)

sub_nn <- matrix(0, nrow(x_test), ncol(y))
oof_nn <- matrix(0, nrow(x_train), ncol(y))
oof_logloss <- c()

for(i in 1:folds){
    
  keras::k_clear_session()  
  print(glue::glue("\n ===== FOLD {i} ===== \n"))
    
  train_idx <- as.numeric(unlist(kf[-i]))
  val_idx   <- kf[[i]]
  
  wide_n_deep <- instance_model()
  
  wide_n_deep %>%
    compile(
      loss = 'categorical_crossentropy',
      optimizer = optimizer_adam(
        lr = 0.0002) )
  
  tictoc::tic()
  wide_n_deep %>% fit(
    x = as.matrix(x_train)[train_idx, ],
    y = y[train_idx, ],
    batch_size = batchsize,
    epochs = epoch,
    validation_data=list(as.matrix(x_train)[val_idx, ],
                        as.matrix(y)[val_idx, ]),
    callbacks = c(early_stopping, plateau)
  )
  tictoc::toc()
    
  oof_nn[val_idx, ] <- predict(wide_n_deep, as.matrix(x_train)[val_idx, ])
  
  sub_nn <- sub_nn + predict(wide_n_deep, as.matrix(x_test)) / folds
  
  oof_loss <- yardstick::mn_log_loss_vec(
    truth = factor(train$target[val_idx]),
    estimate = oof_nn[val_idx, ])
    
  print(glue::glue("\n OOF logloss: {oof_loss} \n"))
  
  oof_logloss <- c(oof_logloss, oof_loss)
  
}
```

```{r}
yardstick::mn_log_loss_vec(
    truth = factor(train$target),
    estimate = oof_nn)

mean(oof_logloss)
```






